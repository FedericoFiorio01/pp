{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# General Information\n",
    "\n",
    "## Data: \n",
    "Renewable Energy Community (REC) energy management https://doi.org/10.1016/j.dib.2022.108590\n",
    "\n",
    "The dataset reports data of the prosumers’ (producers+consumers) energy dynamics for a period of 1 year.\n",
    "The consumption related to 50 households and 1 public building is reported, separated for each consumer and for different appliances.\n",
    "The energy production is reported for the entire REC, consisting in 15 photovoltaic panels.\n",
    "The REC is provided with a fuel cell to store the electricity charge or to provide power, and the buying/selling to the grid happens with a different pricing depending on the time of the day.\n",
    " \n",
    "## Task:\n",
    "To develop an optimal strategy for battery management using Reinforcement Learning in order to maximize the economical reward achieved buying energy from the provider or by selling the photovoltaic-produced energy to the grid.\n",
    "Data from the 1st half of the year can be used to train the predictive models, and data from the 2nd half of the year can be used to evaluate the performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Definizione delle variabili\n",
    "# Definizione dei parametri della batteria e di import/export energia\n",
    "Pbmax = 9 * 0.02  # Potenza batteria massima [MW], ovvero valore massimo della azione di carica batteria\n",
    "Pbmin = -9 * 0.02  # Potenza batteria minima [MW], ovvero valore minimo della azione di carica batteria (scarica massima istantanea)\n",
    "\n",
    "Pimax = 16  # Massima potenza importabile dalla rete [MW]\n",
    "Pemax = 16  # Massima potenza esportabile alla rete [MW]\\\n",
    "\n",
    "Eb = 9 * 0.3  # Capacità batteria: 9 batterie da 0.3 MWh, da considerare come un'unica batteria [MWh]\n",
    "socmax = 0.9  # Limite superiore dello stato di carica della batteria [per unità]\n",
    "socmin = 0.1  # Limite inferiore dello stato di carica della batteria [p.u.]\n",
    "\n",
    "eta_ch = 0.866  # Efficienza di carica della batteria\n",
    "eta_dsc = 0.866  # Efficienza di scarica della batteria\n",
    "\n",
    "\n",
    "#fasce VARIABILI a cui vendo energia a Enel\n",
    "f1_e = 75.83  # €/MWh \n",
    "f2_e = 62.23  # €/MWh\n",
    "f3_e = 51.97  # €/MWh\n",
    "\n",
    "# €/MWh prezzo FISSO a cui compro da Enel\n",
    "Ci = 0.1584 * 1000  \n",
    "\n",
    "DeltaT = 4  # Durata di un intervallo temporale \n",
    "\n",
    "# Calcolo dei prezzi orari di vendita\n",
    "infrasettimanale_e = np.concatenate([\n",
    "    f3_e * np.ones(int((7 - 0) * DeltaT)),\n",
    "    f2_e * np.ones(int((8 - 7) * DeltaT)),\n",
    "    f1_e * np.ones(int((19 - 8) * DeltaT)),\n",
    "    f2_e * np.ones(int((23 - 19) * DeltaT)),\n",
    "    f3_e * np.ones(int((24 - 23) * DeltaT))\n",
    "])\n",
    "\n",
    "sabato_e = np.concatenate([\n",
    "    f3_e * np.ones(int((7 - 0) * DeltaT)),\n",
    "    f2_e * np.ones(int((23 - 7) * DeltaT)),\n",
    "    f3_e * np.ones(int((24 - 23) * DeltaT))\n",
    "])\n",
    "\n",
    "domenica_e = f3_e * np.ones(int(24 * DeltaT))\n",
    "\n",
    "## Fattori da tenere in considerazione per le conseguenze delle azioni scelte dall'agente:\n",
    "# La batteria non può caricarsi a più di socmax*Eb, e scaricarsi a meno di socmin*Eb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creo un df con i prezzi di vendita e di acquisto\n",
    "import pandas as pd\n",
    "Ce = pd.DataFrame({'1': infrasettimanale_e,\n",
    "                   '2': infrasettimanale_e,\n",
    "                '3': infrasettimanale_e,\n",
    "                '4': infrasettimanale_e,\n",
    "                '5': infrasettimanale_e,\n",
    "                '6': sabato_e,\n",
    "                '7': domenica_e})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Ce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download dei dati di consumo e produzione da https://zenodo.org/records/6778401/files/Dataset.xlsx?download=1\n",
    "import os \n",
    "import requests\n",
    "import pandas as pd\n",
    "\n",
    "path = \"data/raw\"\n",
    "url = 'https://zenodo.org/record/6778401/files/Dataset.xlsx?download=1'\n",
    "filename = 'Dataset.xlsx'\n",
    "\n",
    "if not os.path.exists(path):\n",
    "    os.makedirs(path)\n",
    "\n",
    "if not os.path.exists(os.path.join(path, filename)):\n",
    "    r = requests.get(url)\n",
    "    with open(os.path.join(path, filename), 'wb') as f:\n",
    "        f.write(r.content)\n",
    "\n",
    "# Convert for each sheet into xlsx created a csv file\n",
    "xls = pd.ExcelFile(os.path.join(path, filename))\n",
    "for sheet_name in xls.sheet_names:\n",
    "    if not os.path.exists(os.path.join(path, sheet_name + '.csv')):\n",
    "        df = pd.read_excel(xls, sheet_name)\n",
    "        df.to_csv(os.path.join(path, sheet_name + '.csv'), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del xls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment definition\n",
    "\n",
    "The environment is defined by the following parameters:\n",
    "- `battery_capacity`: the battery capacity in kWh\n",
    "- `initial_charge`: the initial charge of the battery in kWh\n",
    "- `charge_power`: the power at which the battery can be charged in kW\n",
    "- `discharge_power`: the power at which the battery can be discharged in kW\n",
    "- `efficiency`: the efficiency of the battery (0 < efficiency < 1)\n",
    "- `buy_price`: the price of buying energy from the provider in €/kWh\n",
    "- `sell_price`: the price of selling energy to the grid in €/kWh\n",
    "- `consumption`: the energy consumption of the prosumers in kWh\n",
    "- `production`: the energy production of the photovoltaic panels in kWh\n",
    "- `state`: the state of the environment, defined by the current charge of the battery and the current hour of the day\n",
    "- `action`: the action taken by the agent, defined by the power at which the battery is charged or discharged\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Objective\n",
    "\n",
    "The objective is to train an agent using Reinforcement Learning to learn the optimal strategy for battery management, in order to maximize the economical reward achieved buying energy from the provider or by selling the photovoltaic-produced energy to the grid.\n",
    "\n",
    "\n",
    "## 1. Dafinizione dell'ambiente\n",
    "- Funzione di carica e scarica della batteria\n",
    "- Funzione di forecast del consumo\n",
    "- Funzione di calcolo del costo energetico\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def carica(Pb, soc):\n",
    "    '''Funzione di carica della batteria\n",
    "    Pb: potenza di carica della batteria [KW]\n",
    "    soc: stato di carica della batteria [MWh]\n",
    "    Eb: capacità della batteria [MWh]\n",
    "    eta_ch: efficienza di carica della batteria\n",
    "    DeltaT: durata di un intervallo temporale [h]\n",
    "    socmax: limite superiore dello stato di carica della batteria [p.u.]\n",
    "    '''\n",
    "\n",
    "    # From Kw to MW\n",
    "    Pb = Pb / 1000\n",
    "    \n",
    "    if Pb > Pbmax:\n",
    "        Pb = Pbmax\n",
    "    \n",
    "    # Per semplicità DeltaT = 1/4 = 0.25 \n",
    "    # Intervallo temporale di 15 minuti\n",
    "    # TODO Check if this is correct\n",
    "    soc = min(soc + Pb * eta_ch * 1 / Eb, socmax) \n",
    "    return soc\n",
    "\n",
    "# Test della funzione di carica\n",
    "soc = 0.5  # Stato di carica iniziale della batteria [p.u.]\n",
    "print(carica(0.1, soc)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funzione di scarica\n",
    "def scarica(Pb, soc):\n",
    "    '''Funzione di scarica della batteria\n",
    "    Pb: potenza di scarica della batteria [KW]\n",
    "    soc: stato di carica della batteria [MWh]\n",
    "    Eb: capacità della batteria [MWh]\n",
    "    eta_dsc: efficienza di scarica della batteria\n",
    "    DeltaT: durata di un intervallo temporale [h]\n",
    "    socmin: limite inferiore dello stato di carica della batteria [p.u.]\n",
    "    '''\n",
    "    # From Kw to MW\n",
    "    Pb = Pb / 1000\n",
    "    \n",
    "    if Pb > (Pbmin * -1):\n",
    "        Pb = Pbmin*-1\n",
    "    \n",
    "    # Per semplicità DeltaT = 1/4 = 0.25 \n",
    "    # Intervallo temporale di 15 minuti\n",
    "    # TODO Check if this is correct\n",
    "    soc = max(soc - Pb * eta_dsc * 1 / Eb, socmin)\n",
    "    return soc\n",
    "\n",
    "# Test della funzione di scarica\n",
    "print(scarica(0.1, soc)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Per semplicità la funzione di forecast del consumo e produzione è implementata come funzione che restituisce il consumo e la produzione attuale, senza considerare la stagionalità o la variazione oraria."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The excel spreadsheet is comprised of the total consumption and total PV production profiles, then all the separated by appliance used and their profiles. All the data were inserted into 15-minute intervals and represent a full year (366 days).\n",
    "\n",
    "“Total Consumers” sheet: total energy consumption profiles, in kW, of the Public building (column A) and each of the 50 residential consumers (columns B to AY) for one year (35,136 periods of 15 minutes);\n",
    "\n",
    "“Total Producers” sheet: total PV energy Production profiles, in kW, by the Public building (column A) and each of the 14 residential producers (columns B to O) for one year (35,136 periods of 15 minutes);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read Total Consumers csv file and print the first 5 rows\n",
    "consumers = pd.read_csv(os.path.join(path, 'Total Consumers.csv'))\n",
    "consumers.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add a 3 new columns to the consumers dataframe with the following values:\n",
    "# - 'day': 1 to 7 for each day of the week\n",
    "# - 'hour': 0 to 23 for each hour of the day\n",
    "# - 'week': 1 to 52 for each week of the year\n",
    "# Remember that the dataset starts on 2019-01-01 and\n",
    "# the first day of the week is a Tuesday\n",
    "# and each row is an 15-minute interval\n",
    "\n",
    "consumers['day'] = (consumers.index // 96) % 7 + 2 # Tuesday is the first day of the week\n",
    "consumers['hour'] = consumers.index % 96 // 4\n",
    "consumers['week'] = (consumers.index // 96) // 7 + 1\n",
    "\n",
    "consumers.head(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the consumers dataframe into a csv file\n",
    "silver_path = \"data/silver\"\n",
    "if not os.path.exists(silver_path):\n",
    "    os.makedirs(silver_path)\n",
    "consumers.to_csv(os.path.join(silver_path, 'TotalConsumption.csv'), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read Total Consumers csv file and print the first 5 rows\n",
    "producers = pd.read_csv(os.path.join(path, 'Total Producers.csv'))\n",
    "producers.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add a 3 new columns to the consumers dataframe with the following values:\n",
    "# - 'day': 1 to 7 for each day of the week\n",
    "# - 'hour': 0 to 23 for each hour of the day\n",
    "# - 'week': 1 to 52 for each week of the year\n",
    "# Remember that the dataset starts on 2019-01-01 and\n",
    "# the first day of the week is a Tuesday\n",
    "# and each row is an 15-minute interval\n",
    "\n",
    "producers['day'] = (producers.index // 96) % 7 + 2 # Tuesday is the first day of the week\n",
    "producers['hour'] = producers.index % 96 // 4\n",
    "producers['week'] = (producers.index // 96) // 7 + 1\n",
    "\n",
    "producers.head(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the consumers dataframe into a csv file\n",
    "silver_path = \"data/silver\"\n",
    "if not os.path.exists(silver_path):\n",
    "    os.makedirs(silver_path)\n",
    "consumers.to_csv(os.path.join(silver_path, 'TotalProductions.csv'), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_window_size = 8\n",
    "window_size = 16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La funzione di predizione prende in input un array di dimensione window_size e restituisce il consumo o la produzione come media dei valori nella window."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(window):\n",
    "    '''Funzione di predizione\n",
    "    window: finestra temporale contenente i consumi o le produzioni passati\n",
    "\n",
    "    return: previsione del consumo o della produzione futura in un np.array di dimensione target_window_size \n",
    "    come media dei valori della finestra temporale passata\n",
    "    '''\n",
    "\n",
    "    #check if the window has the correct size\n",
    "    if len(window) != window_size:\n",
    "        raise ValueError(f'window size must be {window_size}')\n",
    "    \n",
    "    prediction = np.zeros(target_window_size)\n",
    "    for i in range(target_window_size):\n",
    "        prediction[i] = np.mean(window)\n",
    "    \n",
    "    return prediction\n",
    "\n",
    "# Test della funzione di predizione\n",
    "print(predict(consumers['0'].values[:16]))  # 0.00010416666666666667 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definiamo due funzioni che calcolano il valore dell'energia consumata e prodotta in un dato istante di tempo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prezzo_acquisto(Pi):\n",
    "    '''Funzione di calcolo del prezzo di acquisto dell'energia  \n",
    "    Pe: potenza importata dalla rete [MW]\n",
    "\n",
    "    return: prezzo di acquisto dell'energia [€/MWh]\n",
    "    '''\n",
    "\n",
    "    # Calcolo del prezzo di acquisto dell'energia\n",
    "    # Convert Kw to Mw\n",
    "    Pi = Pi / 1000\n",
    "    return Pi * Ci\n",
    "\n",
    "# Test della funzione di calcolo del prezzo di acquisto\n",
    "print(prezzo_acquisto(1))  # 158.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prezzo_di_vendita(day,hour,Pe):\n",
    "    '''Funzione di calcolo del prezzo di vendita dell'energia  \n",
    "    Pe: potenza esportata alla rete [MW]\n",
    "\n",
    "    return: prezzo di vendita dell'energia [€/MWh]\n",
    "    '''\n",
    "\n",
    "    # Calcolo del prezzo di vendita dell'energia\n",
    "    hour = hour * DeltaT\n",
    "    tariffa = Ce[str(day)].loc[hour]\n",
    "    \n",
    "    # Pe from Kw to Mw\n",
    "    Pe = Pe / 1000\n",
    "    return Pe * tariffa\n",
    "\n",
    "# Test della funzione di calcolo del prezzo di vendita\n",
    "print(prezzo_di_vendita(1,1,1))  # 77.97"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stream_data(start_idx, window_size, df):\n",
    "    return df.iloc[start_idx:start_idx+window_size]\n",
    "\n",
    "stream_data(0,window_size=10,df=consumers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definiamo ora il nostro environment. Lo stato dell'environment è definito dalla carica attuale della batteria e dall'ora, dal giorno e dalla settimana.\n",
    "\n",
    "## Stato dell'environment\n",
    "Lo stato dell'environment è definito da:\n",
    "- 'soc': la carica attuale della batteria\n",
    "- 'history': la storia dei consumi e delle produzioni passate (window_size)\n",
    "  - 'hour': l'ora \n",
    "  - 'day': il giorno\n",
    "  - 'week': la settimana\n",
    "  - 'consumption': il consumo\n",
    "  - 'production': la produzione\n",
    "\n",
    "## Azione\n",
    "L'insieme delle azioni possibili è:\n",
    "- 'carica': caricare la batteria\n",
    "- 'scarica': scaricare la batteria\n",
    "- 'compra': comprare energia dalla rete\n",
    "- 'vendi': vendere energia alla rete\n",
    "\n",
    "## Reward\n",
    "Il reward è definita come segue:\n",
    "- se l'azione è vendi, la reward è il prezzo di vendita dell'energia moltiplicato per l'energia venduta \n",
    "- se l'azione è compra, la reward è il prezzo di acquisto dell'energia moltiplicato per l'energia acquistata moltplicato per -10\n",
    "- se la quantità di energia nella batteria più la quantità di energia aquistata è minore della quantità di energia consumata, la reward è -1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gymnasium import Env, spaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gymnasium.utils import seeding\n",
    "\n",
    "class EnergyStorageEnv(Env):\n",
    "    def __init__(self, consumer_id, window_size, target_window_size, seed=None, max_outages_per_episode=1):\n",
    "        self.consumer_id = consumer_id\n",
    "        self.window_size = window_size\n",
    "        self.target_window_size = target_window_size\n",
    "        self._seed(seed)\n",
    "        self.max_outages_per_episode = max_outages_per_episode\n",
    "        self.action_space = spaces.Discrete(3)\n",
    "        self.consumption_data_stream = pd.read_csv(os.path.join(silver_path, \"TotalConsumption.csv\"), usecols=[str(consumer_id),\"day\",\"hour\",\"week\"])\n",
    "        self.production_data_stream = pd.read_csv(os.path.join(silver_path,\"TotalProductions.csv\"), usecols=[str(consumer_id),\"day\",\"hour\",\"week\"])\n",
    "        self.observation_space = spaces.Dict({\n",
    "                # Production data stream\n",
    "                \"prodf\": spaces.Box(low=self.production_data_stream[f\"{consumer_id}\"].min()*self.target_window_size, high=self.production_data_stream[f\"{consumer_id}\"].max()*self.target_window_size, shape=(1,)),\n",
    "                # Consumption data stream\n",
    "                \"consf\": spaces.Box(low=self.consumption_data_stream[f\"{consumer_id}\"].min()*self.target_window_size, high=self.consumption_data_stream[f\"{consumer_id}\"].max()*self.target_window_size, shape=(1,)),\n",
    "                # State of charge\n",
    "                \"soc\": spaces.Box(low=0, high=1, shape=(1,))\n",
    "            }\n",
    "        )\n",
    "        self._reset()\n",
    "\n",
    "    def reset(self, seed=None):\n",
    "        return self._reset(), {}\n",
    "    \n",
    "    def _reset(self):\n",
    "        # Generate a random index to start the data stream using self.seed        \n",
    "        rand_idx = self.np_random.integers(0, len(self.consumption_data_stream)-self.window_size)\n",
    "\n",
    "        self.idx = rand_idx\n",
    "        self.soc = self.np_random.uniform(low=socmin, high=socmax)\n",
    "        self.outage_count = 0\n",
    "        self.Pi = 0\n",
    "        self.Pe = 0\n",
    "        self.prodf = 0\n",
    "        self.consf = 0\n",
    "        self.prod = 0\n",
    "        self.cons = 0\n",
    "        return self._get_obs()\n",
    "    \n",
    "    def step(self, action):\n",
    "        return self._step(action)\n",
    "    \n",
    "    def _seed(self, seed=None):\n",
    "        self.np_random, seed = seeding.np_random(seed)\n",
    "        return [seed]\n",
    "    \n",
    "    def _get_obs(self):\n",
    "\n",
    "        if self.idx < self.window_size:\n",
    "            # Warm up finchè non raggiunge il quantitativo necessario di dati per fare la prediction\n",
    "            pass\n",
    "        elif self.idx >= len(self.consumption_data_stream):\n",
    "            # Se termino lo stream di dati, termino l'episodio\n",
    "            pass\n",
    "        else:\n",
    "            # Il sistema entra in funzione\n",
    "\n",
    "            # History of production and consumption\n",
    "            hist_prod = self.production_data_stream[self.idx-self.window_size:self.idx][f\"{self.consumer_id}\"].values\n",
    "            hist_cons = self.consumption_data_stream[self.idx-self.window_size:self.idx][f\"{self.consumer_id}\"].values\n",
    "\n",
    "            # Forecast of production and consumption\n",
    "            self.prodf = sum(predict(hist_prod))\n",
    "            self.consf = sum(predict(hist_cons))\n",
    "\n",
    "            # Consumo e produziole attuale\n",
    "            self.prod = self.production_data_stream[str(self.consumer_id)].iloc[self.idx]\n",
    "            self.cons = self.consumption_data_stream[str(self.consumer_id)].iloc[self.idx]\n",
    "\n",
    "            # Stato di carica della batteria\n",
    "            # Aggiorno lo stato di carica della batteria\n",
    "            self.soc = carica(self.prod, self.soc)\n",
    "            \n",
    "        self.idx += 1\n",
    "\n",
    "        return {\n",
    "            \"prodf\": np.array([self.prodf]).astype(np.float32), \n",
    "            \"consf\": np.array([self.consf]).astype(np.float32), \n",
    "            \"soc\": np.array([self.soc]).astype(np.float32)\n",
    "        }\n",
    "\n",
    "    \n",
    "    def _step(self, action):\n",
    "        # Switch case for the action as strings\n",
    "        action_mapping = {\n",
    "            0: \"buy\",\n",
    "            1: \"sell\",\n",
    "            2: \"idle\"\n",
    "        }\n",
    "        \n",
    "        action = action_mapping[action]\n",
    "\n",
    "        if action == \"buy\":\n",
    "            # Aggiorno il conto di quanta energia ho acquistato\n",
    "            # In base a quanto prevedo di consumare 'consf'\n",
    "            # Sottraggo poi quanto consumo realmente 'cons'\n",
    "            # Aggiorno il conto di quanta energia ho acquistato\n",
    "            # In base a quanto posso effettivamente importare 'Pimax'\n",
    "            # E Calcolo la differenza tra il consumo previsto/l'energia aquistata e il consumo effettivo\n",
    "            if self.consf >  Pimax * 1000:\n",
    "                self.Pi += Pimax * 1000\n",
    "                diff = Pimax * 1000 - self.cons\n",
    "            else:\n",
    "                self.Pi += self.consf\n",
    "                diff = self.consf - self.cons\n",
    "            \n",
    "            # Se la differenza è positiva, vuol dire che ho consumato meno di quanto previsto\n",
    "            # Quindi ho acquistato energia in eccesso\n",
    "            if (diff) >= 0:\n",
    "                reward = -1\n",
    "            else:\n",
    "                # Se la differenza è negativa, vuol dire che ho consumato più di quanto previsto\n",
    "                # Quindi ho acquistato energia in difetto\n",
    "                # Controllo quindi se posso usere l'energia immagazzinata in precedenza nella batteria\n",
    "                check_soc = scarica(diff, self.soc)\n",
    "                if check_soc == socmin:\n",
    "                    # Non posso usare la batteria perchè è scarica\n",
    "                    reward = -100\n",
    "                    self.outage_count += 1\n",
    "                    self.soc = socmin\n",
    "                else:\n",
    "                    # Posso usare la batteria\n",
    "                    reward = -1\n",
    "                    self.soc = scarica(diff, self.soc)\n",
    "        \n",
    "        elif action == \"sell\":\n",
    "            # Scarico in base a quanto prevedo di produrre 'prodf' e fino a quanto posso esportare 'Pemax'\n",
    "            # E fino a quanta energia ho effettivamente nella batteria 'soc'\n",
    "\n",
    "            if self.prodf > Pemax * 1000:\n",
    "                max_discharge = Pemax * 1000\n",
    "            else:\n",
    "                max_discharge = self.prodf\n",
    "\n",
    "            if self.soc * Eb * 1000 > max_discharge:\n",
    "                # Se ho più energia in batteria di quanto posso scaricare\n",
    "                self.Pe += max_discharge\n",
    "                self.soc = scarica(max_discharge, self.soc)\n",
    "            else:\n",
    "                # Se ho meno energia in batteria di quanto posso scaricare\n",
    "                # Scarico tutta l'energia in batteria fino a Pbmin\n",
    "                diff = self.soc * Eb * 1000 - socmin * Eb * 1000\n",
    "                self.Pe += diff\n",
    "                self.soc = socmin\n",
    "\n",
    "            # Sottraggo poi quanto consumo realmente 'cons'\n",
    "            # Aggiorno il conto di quanta energia ho venduto \n",
    "\n",
    "            check_soc = scarica(self.cons, self.soc)\n",
    "            if check_soc == socmin:\n",
    "                reward = -100\n",
    "                self.soc = socmin\n",
    "                self.outage_count += 1\n",
    "            else:\n",
    "                reward = 1\n",
    "                self.soc = scarica(self.cons, self.soc)\n",
    "\n",
    "        elif action == \"idle\":\n",
    "            check_soc = scarica(self.cons, self.soc)\n",
    "            if check_soc == socmin:\n",
    "                reward = -100\n",
    "                self.soc = socmin\n",
    "                self.outage_count += 1\n",
    "            else:\n",
    "                reward = -1\n",
    "                self.soc = scarica(self.cons, self.soc)\n",
    "\n",
    "\n",
    "        else:\n",
    "            raise ValueError(f\"Invalid action {action}\")\n",
    "        \n",
    "        # Se termino lo stream di dati, termino l'episodio\n",
    "        if (self.idx + 1) > len(self.consumption_data_stream):\n",
    "            done = True\n",
    "        elif self.outage_count > self.max_outages_per_episode:\n",
    "            done = True\n",
    "        else:\n",
    "            done = False\n",
    "            \n",
    "        obs = self._get_obs()\n",
    "    \n",
    "        return obs, reward, done, False, {\n",
    "            \"Pi\": self.Pi,\n",
    "            \"Pe\": self.Pe,\n",
    "            \"soc\": self.soc,\n",
    "            \"prod\": self.prod,\n",
    "            \"cons\": self.cons,\n",
    "            \"prodf\": self.prodf,\n",
    "            \"consf\": self.consf\n",
    "        }\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_strategy(observation):\n",
    "    actios_map = {\n",
    "        0: \"buy\",\n",
    "        1: \"sell\",\n",
    "        2: \"idle\"\n",
    "    }\n",
    "    \n",
    "    return np.random.randint(0, 3)\n",
    "\n",
    "#Test random strategy function\n",
    "env = EnergyStorageEnv(consumer_id=0, window_size=window_size, target_window_size=target_window_size, seed=42)\n",
    "observation, _ = env.reset()\n",
    "print(random_strategy(observation)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the observation space\n",
    "print(env.observation_space)\n",
    "print(observation)\n",
    "print(type(observation))\n",
    "#Check if the observation is of type observation_space\n",
    "env.observation_space.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assert that the observation is in the observation space\n",
    "assert env.observation_space.contains(observation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_observation(observation):\n",
    "    print(\"Prod forecast: {} \\nConsuption Forecasr: {} \\nSOC: {}\".format(\n",
    "          observation[\"prodf\"], observation[\"consf\"], observation[\"soc\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import namedtuple\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "def plot_episode_stats(stats):\n",
    "\n",
    "    # Plot the episode soc over time\n",
    "    fig1 = plt.figure(figsize=(10,5))\n",
    "    soc = pd.Series(stats.episode_soc)\n",
    "    pe = pd.Series(stats.episode_power_exported)\n",
    "    # Double scale plot\n",
    "    ax1 = soc.plot()\n",
    "    ax1.set_ylabel('SOC')\n",
    "    ax2 = pe.plot(secondary_y=True)\n",
    "    ax2.set_ylabel('Power Exported')\n",
    "    ax1.set_xlabel('Episode')\n",
    "    ax1.set_title(\"Episode SOC and Power Exported over Time\")\n",
    "    ax1.legend([\"SOC\"], loc='upper left')\n",
    "    ax2.legend([\"Power Exported\"], loc='upper right')\n",
    "\n",
    "\n",
    "    \n",
    "    # Plot the episode reward moving average with window size of 10\n",
    "    fig2 = plt.figure(figsize=(10,5))\n",
    "    rewards = pd.Series(stats.episode_rewards)\n",
    "    rolling_mean = rewards.rolling(window=10).mean()\n",
    "    ax3 = rolling_mean.plot()\n",
    "    # Compute mean reward and std and plot them\n",
    "    mean = rewards.mean()\n",
    "    std = rewards.std()\n",
    "    ax3.axhline(mean, color='r', linestyle='--', label='Mean Reward')\n",
    "    ax3.axhline(mean + std, color='g', linestyle='--', label='Mean Reward +/- std')\n",
    "    ax3.axhline(mean - std, color='g', linestyle='--')\n",
    "    ax3.legend()\n",
    "    ax3.set_xlabel(\"Episode\")\n",
    "    ax3.set_ylabel(\"Reward\")\n",
    "    ax3.set_title(\"Episode Reward over Time\")\n",
    "    ax3.text(0.5, 0.5, f'Mean Reward: {mean:.2f}', fontsize=12, bbox=dict(facecolor='red', alpha=0.5), transform=ax3.transAxes, horizontalalignment='center', verticalalignment='center')\n",
    "    ax3.text(0.5, 1, f'Mean Reward +/- std: {mean + std:.2f}', fontsize=12, bbox=dict(facecolor='green', alpha=0.5), transform=ax3.transAxes, horizontalalignment='center', verticalalignment='center')\n",
    "\n",
    "\n",
    "    # Plot cumulative rewards\n",
    "    fig3 = plt.figure(figsize=(10,5))\n",
    "    # Calculate the cumulative rewards\n",
    "    cum_rewards = rewards.cumsum()\n",
    "    ax4 = cum_rewards.plot()\n",
    "    ax4.set_xlabel(\"Episode\")\n",
    "    ax4.set_ylabel(\"Cumulative Reward\")\n",
    "    ax4.set_title(\"Cumulative Reward over Time\")\n",
    "    ax4.legend([\"Cumulative Reward\"], loc='upper left')\n",
    "    ax4_1 = rewards.plot(secondary_y=True)\n",
    "    ax4_1.set_ylabel(\"Reward\")\n",
    "    ax4_1.legend([\"Reward\"], loc='upper right')\n",
    "\n",
    "    # Plot the histogram of actions and rewards\n",
    "    fig4, ax5 = plt.subplots(1, 2, figsize=(10,5))\n",
    "    actions = pd.Series(stats.actions)\n",
    "    actions.value_counts().plot(kind='bar', ax=ax5[0])\n",
    "    ax5[0].set_title(\"Action Histogram\")\n",
    "    ax5[0].set_xlabel(\"Actions\")\n",
    "    ax5[0].set_ylabel(\"Frequency\")\n",
    "\n",
    "    rewards = pd.Series(stats.episode_rewards)\n",
    "    rewards.value_counts().plot(kind='bar', ax=ax5[1])\n",
    "    ax5[1].set_title(\"Reward Histogram\")\n",
    "    ax5[1].set_xlabel(\"Rewards\")\n",
    "    ax5[1].set_ylabel(\"Frequency\") \n",
    "\n",
    "    fig5 = plt.figure(figsize=(10,5))\n",
    "    cons = pd.Series(stats.episode_cons)\n",
    "    ax6 = cons.plot()\n",
    "\n",
    "    soc = (pd.Series(stats.episode_soc))\n",
    "    soc_to_kw = soc * Eb\n",
    "    battery_power_var = soc_to_kw.diff().fillna(0)\n",
    "    ax7 = battery_power_var.plot(secondary_y=True)\n",
    "    ax6.set_ylabel('Consumption')\n",
    "    ax7.set_ylabel('Battery Power Variation')\n",
    "    ax6.set_xlabel('Episode')\n",
    "    ax6.legend([\"Consumption\"], loc='upper left')\n",
    "    ax7.legend([\"Battery Power Variation\"], loc='upper right')\n",
    "\n",
    "\n",
    "\n",
    "    return fig1, fig2, fig3, fig4, fig5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Stats = namedtuple(\"Stats\",[\"episode_lengths\", \"episode_rewards\", \"episode_power_imported\", \"episode_power_exported\", \"episode_soc\", \"episode_prod\", \"episode_cons\", \"episode_prodf\", \"episode_consf\", \"actions\"])\n",
    "\n",
    "def simulate_episode(env, policy=random_strategy):\n",
    "    for i_episode in range(1):\n",
    "        observation, _ = env.reset()\n",
    "        done = False\n",
    "        rewards = []\n",
    "        power_imported = []\n",
    "        power_exported = []\n",
    "        soc = []\n",
    "        prod = []\n",
    "        cons = []\n",
    "        actions = []\n",
    "        while not done:\n",
    "            # print_observation(observation)\n",
    "            action = policy(observation)\n",
    "            # Check if Action is valid\n",
    "            assert env.action_space.contains(action)\n",
    "            # print(\"Taking action: {}\".format(action))\n",
    "            observation, reward, done, _, extra_info = env.step(action)\n",
    "            rewards.append(reward)\n",
    "            power_imported.append(extra_info[\"Pi\"])\n",
    "            power_exported.append(extra_info[\"Pe\"])\n",
    "            soc.append(extra_info[\"soc\"])\n",
    "            prod.append(extra_info[\"prod\"])\n",
    "            cons.append(extra_info[\"cons\"])\n",
    "            actions.append(action)\n",
    "            if done:\n",
    "                # print_observation(observation)\n",
    "                #print(\"Game end. Reward: {}\\n\".format(float(reward)))\n",
    "                Stats.episode_rewards = rewards\n",
    "                Stats.episode_power_imported = power_imported\n",
    "                Stats.episode_power_exported = power_exported\n",
    "                Stats.episode_soc = soc\n",
    "                Stats.episode_prod = prod\n",
    "                Stats.episode_cons = cons\n",
    "                Stats.actions = actions\n",
    "                plot_episode_stats(Stats)\n",
    "                break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simulate_episode(env)\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ora implementiamo un agente basato su Q-learning per apprendere la strategia ottimale per la gestione della batteria."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3.common.env_checker import check_env\n",
    "\n",
    "# Add Monitor wrapper to the environment\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "\n",
    "from stable_baselines3 import DQN\n",
    "from stable_baselines3.common.callbacks import EvalCallback, StopTrainingOnRewardThreshold, CallbackList\n",
    "\n",
    "import time\n",
    "import os\n",
    "\n",
    "# Create log dir\n",
    "log_dir = \"logs/\"\n",
    "model_dir = \"models/\"\n",
    "os.makedirs(log_dir, exist_ok=True)\n",
    "\n",
    "# Create a folder with the current date and time\n",
    "timestamp = time.strftime('%Y-%m-%d-%H-%M-%S', time.localtime())\n",
    "log_dir = os.path.join(log_dir, timestamp)\n",
    "model_dir = os.path.join(model_dir, timestamp)\n",
    "\n",
    "os.makedirs(log_dir, exist_ok=True)\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "# Create the train environment\n",
    "# Load the custom environment with the EnergyStorageEnv and check it\n",
    "train_env = EnergyStorageEnv(consumer_id=0, window_size=window_size, target_window_size=target_window_size, seed=42)\n",
    "check_env(train_env)\n",
    "train_env = Monitor(train_env, log_dir+\"/train/train\", info_keywords=(\"Pi\", \"Pe\", \"soc\", \"prod\", \"cons\", \"prodf\", \"consf\"))\n",
    "\n",
    "# Create the evaluation environment\n",
    "eval_env = EnergyStorageEnv(consumer_id=1, window_size=window_size, target_window_size=target_window_size, seed=42)\n",
    "eval_env = Monitor(eval_env, log_dir+\"/eval/eval\", info_keywords=(\"Pi\", \"Pe\", \"soc\", \"prod\", \"cons\", \"prodf\", \"consf\"))\n",
    "\n",
    "# Create the test environment\n",
    "test_env = EnergyStorageEnv(consumer_id=2, window_size=window_size, target_window_size=target_window_size, seed=42)\n",
    "test_env = Monitor(test_env, log_dir+\"/test/test\", info_keywords=(\"Pi\", \"Pe\", \"soc\", \"prod\", \"cons\", \"prodf\", \"consf\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retrain = False\n",
    "best_model_path = \"models/best/best_model.zip\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Create the callback: check every 1000 steps\n",
    "callback_on_best = StopTrainingOnRewardThreshold(reward_threshold=1000, verbose=0)\n",
    "# Deterministic False to have a stochastic evaluation\n",
    "# Cause we evaluate the agent with a different environment \n",
    "# So we need to evaluate the robustness of the agent\n",
    "eval_callback = EvalCallback(eval_env, callback_on_new_best=callback_on_best, eval_freq=10000, best_model_save_path=model_dir, deterministic=False, render=False, verbose=0, warn=False)\n",
    "\n",
    "# Callback list to add the Monitor callback\n",
    "callback_list = CallbackList([eval_callback])\n",
    "\n",
    "# Define the DQN model\n",
    "if retrain:\n",
    "    model = DQN.load(\n",
    "        path=best_model_path, \n",
    "        env=train_env, \n",
    "        verbose=0,\n",
    "        learning_rate=1e-3,  # Learning rate\n",
    "        buffer_size=50000,   # Size of the replay buffer\n",
    "        learning_starts=50000, # Number of steps before training starts\n",
    "        batch_size=32,       # Size of the batch sampled from the replay buffer\n",
    "        target_update_interval=10000, # How often to update the target network\n",
    "        train_freq=1000,        # Update the model every 4 steps\n",
    "        gamma=0.99,          # Discount factor\n",
    "        exploration_fraction=0.6,  # Fraction of total training time where exploration is applied\n",
    "        exploration_initial_eps=1.0, # Initial value of epsilon for exploration\n",
    "        exploration_final_eps=0.02, # Final value of epsilon after exploration\n",
    "        tensorboard_log=log_dir+\"/tensorlogs/\" # Log directory\n",
    "    )\n",
    "else:\n",
    "    model = DQN(\n",
    "        \"MultiInputPolicy\",   # Use a MultiInputPolicy model\n",
    "        train_env,           # Pass the environment\n",
    "        learning_rate=1e-3,  # Learning rate\n",
    "        buffer_size=50000,   # Size of the replay buffer\n",
    "        learning_starts=50000, # Number of steps before training starts\n",
    "        batch_size=32,       # Size of the batch sampled from the replay buffer\n",
    "        target_update_interval=10000, # How often to update the target network\n",
    "        train_freq=1000,        # Update the model every 4 steps\n",
    "        gamma=0.99,          # Discount factor\n",
    "        exploration_fraction=0.6,  # Fraction of total training time where exploration is applied\n",
    "        exploration_initial_eps=1.0, # Initial value of epsilon for exploration\n",
    "        exploration_final_eps=0.02, # Final value of epsilon after exploration\n",
    "        verbose=0,            # Verbose mode to show progress\n",
    "        tensorboard_log=log_dir+\"/tensorlogs/\" # Log directory\n",
    "    )\n",
    "\n",
    "# Train the DQN agent\n",
    "model.learn(total_timesteps=10000, # 10000000\n",
    "            progress_bar=True,\n",
    "            callback=callback_list)  # Specify the number of timesteps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del train_env\n",
    "del eval_env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the evaluation results\n",
    "if retrain:\n",
    "    model = DQN.load(model_dir+\"/best_model.zip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "\n",
    "test_env.reset()\n",
    "mean_reward, std_reward = evaluate_policy(model, \n",
    "                                          test_env, \n",
    "                                          n_eval_episodes=1, # 100 \n",
    "                                          deterministic=False)\n",
    "print(f\"Mean reward: {mean_reward:.2f} +/- {std_reward:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the evaluation results\n",
    "from stable_baselines3.common.results_plotter import load_results, ts2xy\n",
    "\n",
    "results = load_results(log_dir+\"/test/\")\n",
    "x, y = ts2xy(results, 'timesteps')\n",
    "\n",
    "plt.plot(x, y)\n",
    "plt.xlabel('Timesteps')\n",
    "plt.ylabel('Rewards')\n",
    "plt.title('Rewards over time')\n",
    "plt.axhline(mean_reward, color='r', linestyle='--', label='Mean Reward')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Stats = namedtuple(\"Stats\",[\"episode_lengths\", \"episode_rewards\", \"episode_power_imported\", \"episode_power_exported\", \"episode_soc\", \"episode_prod\", \"episode_cons\", \"episode_prodf\", \"episode_consf\", \"actions\"])\n",
    "\n",
    "# Evaluate the trained model\n",
    "obs, _ = test_env.reset()\n",
    "\n",
    "for i_episode in range(1):\n",
    "    observation, _ = test_env.reset()\n",
    "    print_observation(observation)\n",
    "    done = False\n",
    "    rewards = []\n",
    "    power_imported = []\n",
    "    power_exported = []\n",
    "    soc = []\n",
    "    prod = []\n",
    "    cons = []\n",
    "    actions = []\n",
    "    while not done:\n",
    "        # print_observation(observation)\n",
    "        action, _states = model.predict(obs, deterministic=True)\n",
    "        # print(\"Taking action: {}\".format(action))\n",
    "        observation, reward, done, _, extra_info =  test_env.step(action.take(0))\n",
    "        print_observation(observation)\n",
    "        rewards.append(reward)\n",
    "        power_imported.append(extra_info[\"Pi\"])\n",
    "        power_exported.append(extra_info[\"Pe\"])\n",
    "        soc.append(extra_info[\"soc\"])\n",
    "        prod.append(extra_info[\"prod\"])\n",
    "        cons.append(extra_info[\"cons\"])\n",
    "        actions.append(action)\n",
    "\n",
    "        if done:\n",
    "            # print_observation(observation)\n",
    "            #print(\"Game end. Reward: {}\\n\".format(float(reward)))\n",
    "            Stats.episode_rewards = rewards\n",
    "            Stats.episode_power_imported = power_imported\n",
    "            Stats.episode_power_exported = power_exported\n",
    "            Stats.episode_soc = soc\n",
    "            Stats.episode_prod = prod\n",
    "            Stats.episode_cons = cons\n",
    "            Stats.actions = actions\n",
    "            plot_episode_stats(Stats)\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_env.close()\n",
    "del test_env"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Osservazioni\n",
    "Dai primi esperimenti condotti emerge come l'agente predilige unicamente vendere costantemente l'energia immagazzinata arrivando infine alla conclusione dell'episodio in via prematura (power outage)(circa 200 step). Performando dunque nettamente peggio rispetto una policy completamente random in termini di lunghezza dell'episodio ma non in reward ottenuta.\n",
    "\n",
    "Nota: l'environment viene inizializzato con una battery state of charge campionata da una  distribuzione uniforme tra socmin e socmax.\n",
    "\n",
    "### Metrics\n",
    "\n",
    "Tabella con le metriche da monitorare:\n",
    "\n",
    "\n",
    "- `reward`: la reward ottenuta dall'agente\n",
    "- `episode_length`: la lunghezza dell'episodio\n",
    "\n",
    "| \\ | Training | Validation |\n",
    "|---------|-------------|----------------|\n",
    "| `episode_mean_reward`| ![image info](imgs/exp_train_2024-09-12-10-02-21_mean_rew.png) | ![image info](imgs/exp_valid_2024-09-12-10-02-21_mean_rew.png) |\n",
    "| `episode_mean_length` | ![image info](imgs/exp_train_2024-09-12-10-02-21_mean_len.png) | ![image info](imgs/exp_valid_2024-09-12-10-02-21_mean_len.png) |\n",
    "\n",
    "\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Considerazioni post revisione\n",
    "\n",
    "All'inizio è corretto che l'agente venda energia senza curarsi di cosa accade, ma via via dovrebbe capire che se arriva al power outage è un grosso problema e cercare di evitarlo nelle iterazioni successive.\n",
    "\n",
    "Una eventuale variazione può essere aggiungere fin da subito una reward che premi il nostro agente quando la batteria è intorno alla metà della sua carica, e che lo penalizzi quando la carica scende o sale troppo (nel primo caso perché sta esagerando nel vendere, nel secondo caso nel tenersi energia che potrebbe vendere). In questo modo, l'agente dovrebbe dare priorità alla conservazione della batteria.\n",
    "\n",
    "Ad esempio:\n",
    "\n",
    "> reward = - k ( soc - 40) (soc - 60)\n",
    "\n",
    "può essere un tentativo, dove k è una costante positiva (un numero compreso tra 0 e 1, come 0.1, 0.5 o 1). Questo reward verrebbe dato in aggiunta a tutti quelli definito sopra.\n",
    "\n",
    "Ovviamente la cosa importante nel RL è che:\n",
    "nel tempo il nostro agente migliori la propria performance (quindi dovrebbe testarne il comportamento al crescere del numero di iterazioni di addestramento);\n",
    "impari una policy che, non per forza all'inizio ma almeno dopo un po', sia migliore di quella random."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definisci la funzione reward come -K(soc - 0.4)(soc - 0.6) e plotta la funzione con k variabile tra 0 e 1\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def reward(soc, k):\n",
    "    return - k*(soc - 0.4)*(soc - 0.6)\n",
    "\n",
    "soc = np.linspace(0, 1, 100)\n",
    "k = np.linspace(0, 1, 10)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "for i in k:\n",
    "    ax.plot(soc, reward(soc, i), label=f'k={i}')\n",
    "ax.set_xlabel('SOC')\n",
    "ax.set_ylabel('Reward')\n",
    "ax.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gymnasium import Wrapper\n",
    "from stable_baselines3.common.env_checker import check_env\n",
    "\n",
    "class NewRewardStrategyWrapper(Wrapper):\n",
    "    def __init__(self, env):\n",
    "        super(NewRewardStrategyWrapper, self).__init__(env)\n",
    "\n",
    "    def step(self, action):\n",
    "        # Week cycle counter\n",
    "        self.weekcycle += 1\n",
    "\n",
    "        obs, reward, done, _ , info = self.env.step(action)\n",
    "        # Modifica il valore ritornato dalla funzione step\n",
    "        soc = state[0][\"soc\"][0]\n",
    "\n",
    "        reward = reward - 0.7 * (soc - 0.4) * (soc - 0.6)\n",
    "\n",
    "        # Aggiungi un reward bonus ogni 96*7 step \n",
    "        # Dunque ogni settimana\n",
    "        if self.weekcycle % (96*7):\n",
    "            reward += 3\n",
    "\n",
    "        # Aggiungi un reward bonus ogni 96*30 step\n",
    "        # Dunque ogni mese\n",
    "        if self.weekcycle % (96*30):\n",
    "            reward += 6\n",
    "        \n",
    "        return obs, float(reward), done, _ , info\n",
    "     \n",
    "    def reset(self, **kwargs):\n",
    "        self.weekcycle = 0\n",
    "        return self.env.reset(**kwargs)\n",
    "\n",
    "# Utilizzo del wrapper\n",
    "env = EnergyStorageEnv(consumer_id=1, window_size=window_size, target_window_size=target_window_size, seed=42)\n",
    "wrapped_env = NewRewardStrategyWrapper(env)\n",
    "\n",
    "# Esempio di utilizzo dell'ambiente wrappato\n",
    "state = wrapped_env.reset()\n",
    "print(state)\n",
    "action = wrapped_env.action_space.sample()\n",
    "print(action)\n",
    "obs, reward, done, _ , info = wrapped_env.step(action)\n",
    "print(f\"Observation: {obs},\\nReward: {reward},\\nDone: {done},\\nInfo: {info}\")\n",
    "\n",
    "check_env(wrapped_env)\n",
    "env.close()\n",
    "del env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3.common.env_checker import check_env\n",
    "\n",
    "# Add Monitor wrapper to the environment\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "\n",
    "from stable_baselines3 import DQN\n",
    "from stable_baselines3.common.callbacks import EvalCallback, StopTrainingOnRewardThreshold, CallbackList\n",
    "\n",
    "import time\n",
    "import os\n",
    "\n",
    "# Create log dir\n",
    "log_dir = \"logs/\"\n",
    "model_dir = \"models/\"\n",
    "os.makedirs(log_dir, exist_ok=True)\n",
    "\n",
    "# Create a folder with the current date and time\n",
    "timestamp = time.strftime('%Y-%m-%d-%H-%M-%S', time.localtime())\n",
    "log_dir = os.path.join(log_dir, timestamp)\n",
    "model_dir = os.path.join(model_dir, timestamp)\n",
    "\n",
    "os.makedirs(log_dir, exist_ok=True)\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "# Create the train environment\n",
    "# Load the custom environment with the EnergyStorageEnv and check it\n",
    "train_env = EnergyStorageEnv(consumer_id=0, window_size=window_size, target_window_size=target_window_size, seed=42)\n",
    "train_env = NewRewardStrategyWrapper(train_env)\n",
    "train_env = Monitor(train_env, log_dir+\"/train/train\", info_keywords=(\"Pi\", \"Pe\", \"soc\", \"prod\", \"cons\", \"prodf\", \"consf\"))\n",
    "check_env(train_env)\n",
    "\n",
    "# Create the evaluation environment\n",
    "eval_env = EnergyStorageEnv(consumer_id=1, window_size=window_size, target_window_size=target_window_size, seed=42)\n",
    "eval_env = NewRewardStrategyWrapper(eval_env)\n",
    "eval_env = Monitor(eval_env, log_dir+\"/eval/eval\", info_keywords=(\"Pi\", \"Pe\", \"soc\", \"prod\", \"cons\", \"prodf\", \"consf\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the callback: check every 1000 steps\n",
    "callback_on_best = StopTrainingOnRewardThreshold(reward_threshold=500000, verbose=0)\n",
    "# Deterministic False to have a stochastic evaluation\n",
    "# Cause we evaluate the agent with a different environment \n",
    "# So we need to evaluate the robustness of the agent\n",
    "eval_callback = EvalCallback(eval_env, \n",
    "                             callback_on_new_best=callback_on_best, \n",
    "                             eval_freq=100000, \n",
    "                             n_eval_episodes=5,\n",
    "                             best_model_save_path=model_dir, \n",
    "                             deterministic=False, \n",
    "                             render=False, verbose=0, warn=False)\n",
    "\n",
    "# Learning rate schedule\n",
    "# A schedule takes the remaining progress as input and returns the learning rate\n",
    "def learning_rate_schedule(progress):\n",
    "    # Se manca ancora l'80% del training, il learning rate è 1e-5\n",
    "    if (1-progress) <= 0.7:\n",
    "        return 1e-5\n",
    "    # Se manca ancora il 50% del training, il learning rate è 5e-4\n",
    "    elif (1-progress) <= 0.9:\n",
    "        return 5e-6\n",
    "    # Altrimenti il learning rate è 1e-3\n",
    "    else:\n",
    "        return 1e-6\n",
    "\n",
    "    # if progress < 0.5:\n",
    "    #     return 1e-3\n",
    "    # elif progress < 0.8:\n",
    "    #     return 5e-4\n",
    "    # else:\n",
    "    #     return 1e-5\n",
    "\n",
    "# Callback list to add the Monitor callback\n",
    "callback_list = CallbackList([eval_callback])\n",
    "\n",
    "# Define the DQN model\n",
    "model = DQN(\n",
    "    \"MultiInputPolicy\",   # Use a MultiInputPolicy model\n",
    "    train_env,           # Pass the environment\n",
    "    learning_rate=learning_rate_schedule,  # Learning rate\n",
    "    buffer_size=900000,   # Size of the replay buffer\n",
    "    learning_starts=500000, # Number of steps before training starts\n",
    "    batch_size=32,       # Size of the batch sampled from the replay buffer\n",
    "    target_update_interval=10000, # How often to update the target network\n",
    "    train_freq=(15, \"episode\"), # 10000,        # Update the model every 4 steps\n",
    "    gamma=0.7,          # Discount factor\n",
    "    max_grad_norm=10,   # Max gradient norm\n",
    "    exploration_fraction=0.6,  # Fraction of total training time where exploration is applied\n",
    "    exploration_initial_eps=1.0, # Initial value of epsilon for exploration\n",
    "    exploration_final_eps=0.02, # Final value of epsilon after exploration\n",
    "    verbose=0,            # Verbose mode to show progress\n",
    "    tensorboard_log=log_dir+\"/tensorlogs/\", # Log directory\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "# Train the DQN agent\n",
    "model.learn(total_timesteps=15000000,\n",
    "            progress_bar=True,\n",
    "            callback=callback_list)  # Specify the number of timesteps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the test environment\n",
    "test_env = EnergyStorageEnv(consumer_id=2, window_size=window_size, target_window_size=target_window_size, seed=42)\n",
    "test_env = NewRewardStrategyWrapper(test_env)\n",
    "test_env = Monitor(test_env, log_dir+\"/test/test\",info_keywords=(\"Pi\", \"Pe\", \"soc\", \"prod\", \"cons\", \"prodf\", \"consf\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "\n",
    "test_env.reset()\n",
    "mean_reward, std_reward = evaluate_policy(model, test_env, n_eval_episodes=50, deterministic=False)\n",
    "print(f\"Mean reward: {mean_reward:.2f} +/- {std_reward:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the evaluation results\n",
    "from stable_baselines3.common.results_plotter import load_results, ts2xy\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "results = load_results(log_dir+\"/test\")\n",
    "x, y = ts2xy(results, 'timesteps')\n",
    "\n",
    "plt.plot(x, y)\n",
    "plt.xlabel('Timesteps')\n",
    "plt.ylabel('Rewards')\n",
    "plt.title('Rewards over time')\n",
    "plt.axhline(mean_reward, color='r', linestyle='--', label='Mean Reward')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Stats = namedtuple(\"Stats\",[\"episode_lengths\", \"episode_rewards\", \"episode_power_imported\", \"episode_power_exported\", \"episode_soc\", \"episode_prod\", \"episode_cons\", \"episode_prodf\", \"episode_consf\", \"actions\"])\n",
    "\n",
    "# Evaluate the trained model\n",
    "obs, _ = test_env.reset()\n",
    "\n",
    "for i_episode in range(1):\n",
    "    observation, _ = test_env.reset()\n",
    "    print_observation(observation)\n",
    "    done = False\n",
    "    rewards = []\n",
    "    power_imported = []\n",
    "    power_exported = []\n",
    "    soc = []\n",
    "    prod = []\n",
    "    cons = []\n",
    "    actions = []\n",
    "    while not done:\n",
    "        # print_observation(observation)\n",
    "        action, _states = model.predict(obs, deterministic=True)\n",
    "        # print(\"Taking action: {}\".format(action))\n",
    "        observation, reward, done, _, extra_info =  test_env.step(action.take(0))\n",
    "        print_observation(observation)\n",
    "        rewards.append(reward)\n",
    "        power_imported.append(extra_info[\"Pi\"])\n",
    "        power_exported.append(extra_info[\"Pe\"])\n",
    "        soc.append(extra_info[\"soc\"])\n",
    "        prod.append(extra_info[\"prod\"])\n",
    "        cons.append(extra_info[\"cons\"])\n",
    "        actions.append(action)\n",
    "\n",
    "        if done:\n",
    "            # print_observation(observation)\n",
    "            #print(\"Game end. Reward: {}\\n\".format(float(reward)))\n",
    "            Stats.episode_rewards = rewards\n",
    "            Stats.episode_power_imported = power_imported\n",
    "            Stats.episode_power_exported = power_exported\n",
    "            Stats.episode_soc = soc\n",
    "            Stats.episode_prod = prod\n",
    "            Stats.episode_cons = cons\n",
    "            Stats.actions = actions\n",
    "            plot_episode_stats(Stats)\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Osservazioni\n",
    "Dai primi esperimenti condotti emerge come l'agente predilige unicamente vendere costantemente l'energia immagazzinata arrivando infine alla conclusione dell'episodio in via prematura (power outage)(circa 200 step). Performando dunque nettamente peggio rispetto una policy completamente random in termini di lunghezza dell'episodio ma non in reward ottenuta.\n",
    "\n",
    "Nota: l'environment viene inizializzato con una battery state of charge campionata da una  distribuzione uniforme tra socmin e socmax.\n",
    "\n",
    "### Metrics\n",
    "\n",
    "Tabella con le metriche da monitorare:\n",
    "\n",
    "\n",
    "- `reward`: la reward ottenuta dall'agente\n",
    "- `episode_length`: la lunghezza dell'episodio\n",
    "\n",
    "| \\ | Training | Validation |\n",
    "|---------|-------------|----------------|\n",
    "| `episode_mean_reward`| ![image info](imgs/exp_train_2024-09-12-10-02-21_mean_rew.png) | ![image info](imgs/exp_valid_2024-09-12-10-02-21_mean_rew.png) |\n",
    "| `episode_mean_length` | ![image info](imgs/exp_train_2024-09-12-10-02-21_mean_len.png) | ![image info](imgs/exp_valid_2024-09-12-10-02-21_mean_len.png) |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rl-policy-rec-powerconsumption",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
